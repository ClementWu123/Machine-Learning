{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fAntA-NYeASN-BTvjhEb5DJoVO8WBvTy","timestamp":1697593327929}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CIS 5200: Machine Learning\n","## Homework 2"],"metadata":{"id":"lIvXJ8GV9Lq9"}},{"cell_type":"code","execution_count":99,"metadata":{"id":"a4HTcwb59Eaw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697607295258,"user_tz":240,"elapsed":141,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"3daa5cc7-07e8-494e-fe8d-cc95a12e88f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO, OK] Google Colab.\n"]}],"source":["import os\n","import sys\n","\n","# For autograder only, do not modify this cell.\n","# True for Google Colab, False for autograder\n","NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n","if NOTEBOOK:\n","    print(\"[INFO, OK] Google Colab.\")\n","else:\n","    print(\"[INFO, OK] Autograder.\")\n","    sys.exit()"]},{"cell_type":"markdown","source":["## Penngrader setup"],"metadata":{"id":"e1-lu5IJ9R0G"}},{"cell_type":"code","source":["# %%capture\n","!pip install penngrader-client"],"metadata":{"id":"o4K9_rWa9TKJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697607301534,"user_tz":240,"elapsed":4846,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"241ad211-c426-4a1e-dbcc-cf340d4df762"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: penngrader-client in /usr/local/lib/python3.10/dist-packages (0.5.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from penngrader-client) (0.3.7)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from penngrader-client) (6.0.1)\n"]}]},{"cell_type":"code","source":["%%writefile config.yaml\n","grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n","grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"],"metadata":{"id":"nO2IH6cO9VR2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697607304969,"user_tz":240,"elapsed":119,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"00109819-c972-4afd-e5ff-d9cc035b9d0b"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting config.yaml\n"]}]},{"cell_type":"code","source":["from penngrader.grader import PennGrader\n","\n","# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n","# TO ASSIGN POINTS TO YOU IN OUR BACKEND\n","STUDENT_ID = 72249835 # YOUR PENN-ID GOES HERE AS AN INTEGER #\n","SECRET = STUDENT_ID\n","\n","grader = PennGrader('config.yaml', 'CIS5200_FALL_2023_HW2', STUDENT_ID, SECRET)"],"metadata":{"id":"qt233qEO9YfK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697607306750,"user_tz":240,"elapsed":111,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"3a5927db-e727-4f0a-fb85-a0568addec0b"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["PennGrader initialized with Student ID: 72249835\n","\n","Make sure this correct or we will not be able to store your grade\n"]}]},{"cell_type":"markdown","source":["# Dataset: Wine Quality Prediction\n","\n","Some research on blind wine tasting has suggested that [people cannot taste the difference between ordinary and pricy wine brands](https://phys.org/news/2011-04-expensive-inexpensive-wines.html). Indeed, even experienced tasters may be as consistent as [random numbers](https://www.seattleweekly.com/food/wine-snob-scandal/).\n","\n","In this problem set, we will train some simple linear models to predict wine quality. We'll be using the data from [this repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality) for both the classification and regression tasks. The following cells will download and set up the data for you."],"metadata":{"id":"MipzHX9n9Zzo"}},{"cell_type":"code","source":["%%capture\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names"],"metadata":{"id":"LzHV_XSm9bY4","executionInfo":{"status":"ok","timestamp":1697607310627,"user_tz":240,"elapsed":1589,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"execution_count":103,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import torch\n","\n","red_df = pd.read_csv('winequality-red.csv', delimiter=';')\n","\n","X = torch.from_numpy(red_df.drop(columns=['quality']).to_numpy())\n","y = torch.from_numpy(red_df['quality'].to_numpy())\n","\n","# Split data into train/test splits\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42)\n","\n","# Normalize the data to have zero mean and standard deviation,\n","# and add bias term\n","mu, sigma = X_train.mean(0), X_train.std(0)\n","X_train, X_test = [ torch.cat([((x-mu)/sigma).float(), torch.ones(x.size(0),1)], dim=1)\n","                    for x in [X_train, X_test]]\n","\n","# Transform labels to {-1,1} for logistic regression\n","y_binary_train, y_binary_test = [ (torch.sign(y - 5.5)).long()\n","                                  for y in [y_train, y_test]]\n","y_regression_train, y_regression_test = [ y.float() for y in [y_train, y_test]]"],"metadata":{"id":"0nQQ3oPx9ezZ","executionInfo":{"status":"ok","timestamp":1697607311792,"user_tz":240,"elapsed":124,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":["# 1. Logistic Regression\n","\n","In this first problem, you will implement a logistic regression classifier to classify good wine (`y=1`) from bad wine (`y=-1`). Your professor has arbitrarily decided that good wine has a score of at least 5.5. The classifier is split into the following components:\n","\n","1. Loss (3pts) & gradient (3pts) - given a batch of examples $X$ and labels $y$ and weights for the logistic regression classifier, compute the batched logistic loss and gradient of the loss with *with respect to the model parameters $w$*. Note that this is slightly different from the gradient in Homework 0, which was with respect to the sample $X$.\n","2. Fit (2pt) - Given a loss function and data, find the weights of an optimal logistic regression model that minimizes the logistic loss\n","3. Predict (3pts) - Given the weights of a logistic regression model and new data, predict the most likely class\n","\n","We provide an generic gradient-based optimizer for you which minimizes the logistic loss function, you can call it with `LogisticOptimizer().optimize(X,y)`. It does not need any parameter adjustment.\n","\n","Hint: The optimizer will minimize the logistic loss. So this value of this loss should be decreasing over iterations."],"metadata":{"id":"iMi0QqDI9jK8"}},{"cell_type":"code","source":["class LogisticOptimizer:\n","    @staticmethod\n","    def logistic_loss(X, y, w):\n","        # Given a batch of samples and labels, and the weights of a logistic\n","        # classifier, compute the batched logistic loss.\n","        #\n","        # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n","        #     of dimension d\n","        #\n","        # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n","        #\n","        # w := Tensor(float) of size(d,) --- This is the weights of a logistic\n","        #     classifer.\n","        #\n","        # Return := Tensor of size (m,) --- This is the logistic loss for each\n","        #     example.\n","\n","        # Fill in the rest\n","        loss = torch.log(1 + torch.exp(-y * (X @ w)))\n","        return loss\n","\n","    @staticmethod\n","    def logistic_gradient(X, y, w):\n","        # Given a batch of samples and labels, compute the batched gradient of\n","        # the logistic loss.\n","        #\n","        # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n","        #     of dimension d\n","        #\n","        # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n","        #\n","        # w := Tensor(float) of size(d,) --- This is the weights of a logistic\n","        #     classifer.\n","        #\n","        # Return := Tensor of size (m,d) --- This is the logistic gradient for each\n","        #     example.\n","        #\n","        # Hint: A very similar gradient was calculated in Homework 0.\n","        # However, that was the sample gradient (with respect to X), whereas\n","        # what we need here is the parameter gradient (with respect to w).\n","\n","        # Fill in the rest\n","        gradient =  ((-y * torch.exp(-y * (X @ w))) / (1 + torch.exp(-y * (X @ w))))[:,None] * X\n","        return gradient\n","\n","    def optimize(self, X, y, niters=100):\n","        # Given a dataset of examples and labels, minimizes the logistic loss\n","        # using standard gradient descent.\n","        #\n","        # This optimizer is written for you, and you only need to implement the\n","        # logistic loss and gradient functions above.\n","        #\n","        # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n","        #     of dimension d\n","        #\n","        # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n","        #\n","        # Return := Tensor of size(d,) --- This is the fitted weights of a\n","        #     logistic regression model\n","\n","        m,d = X.size()\n","        w = torch.zeros(d)\n","        print('Optimizing logistic function...')\n","        for i in range(niters):\n","            loss = self.logistic_loss(X,y,w).mean()\n","            grad = self.logistic_gradient(X,y,w).mean(0)\n","            w -= grad\n","            if i % 50 == 0:\n","                print(i, loss.item())\n","        print('Optimizing done.')\n","        return w\n","\n","def logistic_fit(X, y, optimizer=LogisticOptimizer):\n","    # Given a dataset of examples and labels, fit the weights of the logistic\n","    # regression classifier using the provided loss function and optimizer\n","    #\n","    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n","    #     of dimension d\n","    #\n","    # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n","    #\n","    # Return := Tensor of size (d,) --- This is the fitted weights of the\n","    #     logistic regression model\n","    opt = LogisticOptimizer()\n","\n","    # Fill in the rest\n","    weight = opt.optimize(X, y)\n","\n","    return weight\n","\n","def logistic_predict(X, w):\n","    # Given a dataset of examples and fitted weights for a logistic regression\n","    # classifier, predict the class\n","    #\n","    # X := Tensor(float) of size(m,d) --- This is a batch of m examples of\n","    #    dimension d\n","    #\n","    # w := Tensor(float) of size (d,) --- This is the fitted weights of the\n","    #    logistic regression model\n","    #\n","    # Return := Tensor of size (m,) --- This is the predicted classes {-1,1}\n","    #    for each example\n","    #\n","    # Hint: Remember that logistic regression expects a label in {-1,1}, and\n","    # not {0,1}\n","\n","    # Fill in the rest\n","    pred = torch.sign(X @ w)\n","\n","    return pred\n"],"metadata":{"id":"8pN0TUK19m7x","executionInfo":{"status":"ok","timestamp":1697607313496,"user_tz":240,"elapsed":137,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["# Test your code on the wine dataset!\n","# How does your solution compare to a random linear classifier?\n","# Your solution should get around 75% accuracy on the test set.\n","torch.manual_seed(42)\n","\n","d = X_train.size(1)\n","logistic_weights = {\n","    'zero': torch.zeros(d),\n","    'random': torch.randn(d),\n","    'fitted': logistic_fit(X_train, y_binary_train)\n","}\n","\n","for k,w in logistic_weights.items():\n","    yp_binary_train = logistic_predict(X_train, w)\n","    acc_train = (yp_binary_train == y_binary_train).float().mean()\n","\n","    print(f'Train accuracy [{k}]: {acc_train.item():.2f}')\n","\n","    yp_binary_test = logistic_predict(X_test, w)\n","    acc_test = (yp_binary_test == y_binary_test).float().mean()\n","\n","    print(f'Test accuracy [{k}]: {acc_test.item():.2f}')"],"metadata":{"id":"cbjOw0Za9pu5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697607316305,"user_tz":240,"elapsed":121,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"0297d0d0-9aef-4895-f1a0-12efc5578b3b"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimizing logistic function...\n","0 0.6931473016738892\n","50 0.518214225769043\n","Optimizing done.\n","Train accuracy [zero]: 0.00\n","Test accuracy [zero]: 0.00\n","Train accuracy [random]: 0.54\n","Test accuracy [random]: 0.53\n","Train accuracy [fitted]: 0.75\n","Test accuracy [fitted]: 0.75\n"]}]},{"cell_type":"markdown","source":["### Autograder\n","Be sure you can pass the following four test cases!"],"metadata":{"id":"OC3HusWT9sdT"}},{"cell_type":"code","source":["grader.grade(test_case_id = 'logistic_loss', answer = LogisticOptimizer.logistic_loss)\n","grader.grade(test_case_id = 'logistic_gradient', answer = LogisticOptimizer.logistic_gradient)\n","grader.grade(test_case_id = 'logistic_fit', answer = logistic_fit)\n","grader.grade(test_case_id = 'logistic_predict', answer = logistic_predict)"],"metadata":{"id":"iBgHs8r-9tS_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697607319222,"user_tz":240,"elapsed":1325,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"ff5c99cd-7714-4f44-c8e9-79d0e13f4472"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct! You earned 3/3 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 3/3 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 2/2 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 3/3 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n"]}]},{"cell_type":"markdown","source":["# 2. Linear Regression with Ridge Regression\n","\n","In this second problem, you'll implement a linear regression model. Similarly to the first problem, implement the following functions:\n","\n","1. Loss (3pts) - Given a batch of examples $X$ and labels $y$, compute the batched mean squared error loss for a linear model with weights $w$.\n","2. Fit (4pts) - Given a batch of examples $X$ and labels $y$, find the weights of the optimal linear regression model\n","3. Predict (3pts) - Given the weights $w$ of a linear regression model and new data $X$, predict the most likely label\n","\n","This time, you are not given an optimizer for the fitting function since this problem has an analytic solution. Make sure to test your solution with non-zero ridge regression parameters."],"metadata":{"id":"FcLy1Sg09vFk"}},{"cell_type":"code","source":["def regression_loss(X, y, w):\n","    # Given a batch of linear regression outputs and true labels, compute\n","    # the batch of squared error losses. This is *without* the ridge\n","    # regression penalty.\n","    #\n","    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n","    #     of dimension d\n","    #\n","    # y := Tensor(int) of size (m,) --- This is a batch of m real-valued labels\n","    #\n","    # w := Tensor(float) of size(d,) --- This is the weights of a linear\n","    #     classifer\n","    #\n","    # Return := Tensor of size (m,) --- This is the squared loss for each\n","    #     example\n","\n","    # Fill in the rest\n","    loss = (y - X @ w)**2\n","\n","    return loss\n","\n","def regression_fit(X, y, ridge_penalty=1.0):\n","    # Given a dataset of examples and labels, fit the weights of the linear\n","    # regression classifier using the provided loss function and optimizer\n","    #\n","    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n","    #     of dimension d\n","    #\n","    # y := Tensor(float) of size (m,) --- This is a batch of m real-valued\n","    #     labels\n","    #\n","    # ridge_penalty := float --- This is the parameter for ridge regression\n","    #\n","    # Return := Tensor of size (d,) --- This is the fitted weights of the\n","    #     linear regression model\n","    #\n","    # Fill in the rest\n","    m,d = X.size()\n","    I = torch.eye(d)\n","    weight = torch.inverse(X.T @ X + ridge_penalty * m * I) @ X.T @ y\n","    return weight\n","\n","\n","def regression_predict(X, w):\n","    # Given a dataset of examples and fitted weights for a linear regression\n","    # classifier, predict the label\n","    #\n","    # X := Tensor(float) of size(m,d) --- This is a batch of m examples of\n","    #    dimension d\n","    #\n","    # w := Tensor(float) of size (d,) --- This is the fitted weights of the\n","    #    linear regression model\n","    #\n","    # Return := Tensor of size (m,) --- This is the predicted real-valued labels\n","    #    for each example\n","    #\n","    # Fill in the rest\n","    pred = X @ w\n","\n","    return pred"],"metadata":{"id":"_kRGPd2g9xFe","executionInfo":{"status":"ok","timestamp":1697607321055,"user_tz":240,"elapsed":138,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"execution_count":108,"outputs":[]},{"cell_type":"code","source":["# Test your code on the wine dataset!\n","# How does your solution compare to a random linear classifier?\n","# Your solution should get an average squard error of about 8.6 test set.\n","torch.manual_seed(42)\n","\n","d = X_train.size(1)\n","regression_weights = {\n","    'zero': torch.zeros(d),\n","    'random': torch.randn(d),\n","    'fitted': regression_fit(X_train, y_regression_train)\n","}\n","\n","for k,w in regression_weights.items():\n","    yp_regression_train = regression_predict(X_train, w)\n","    squared_loss_train = regression_loss(X_train, y_regression_train, w).mean()\n","\n","    print(f'Train accuracy [{k}]: {squared_loss_train.item():.2f}')\n","\n","    yp_regression_test = regression_predict(X_test, w)\n","    squared_loss_test = regression_loss(X_test, y_regression_test, w).mean()\n","\n","    print(f'Test accuracy [{k}]: {squared_loss_test.item():.2f}')"],"metadata":{"id":"AjRqOGqQ90vg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697607323845,"user_tz":240,"elapsed":127,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"c7fc3bed-74b2-4647-ceec-0a4dc64d4fe7"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy [zero]: 32.28\n","Test accuracy [zero]: 32.97\n","Train accuracy [random]: 29.64\n","Test accuracy [random]: 29.55\n","Train accuracy [fitted]: 8.37\n","Test accuracy [fitted]: 8.60\n"]}]},{"cell_type":"markdown","source":["### Autograder\n","Be sure you can pass the following three test cases!"],"metadata":{"id":"4DqhMV2b92c8"}},{"cell_type":"code","source":["grader.grade(test_case_id = 'regression_loss', answer = regression_loss)\n","grader.grade(test_case_id = 'regression_fit', answer = regression_fit)\n","grader.grade(test_case_id = 'regression_predict', answer = regression_predict)"],"metadata":{"id":"TyggyMXt92vv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697607332290,"user_tz":240,"elapsed":853,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"4e6e348f-2781-41cb-c049-6e9dea77a98e"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct! You earned 3/3 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 4/4 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 3/3 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n"]}]},{"cell_type":"markdown","source":["# SVM and Gradient Descent (10 pts)\n","\n","In this problem, you'll implement (soft margin) support vector machines with gradient descent.\n","+ (2pts) Calculate the objective of the Soft SVM (primal)\n","+ (2pts) Calculate the gradient of the Soft SVM objective\n","+ (4pts) Implement a gradient descent optimizer. Your solution needs to converge to an accurate enough answer.\n","+ (2pts) Make predictions with the Soft SVM\n","\n","Tips:\n","- This assignment is more freeform than previous ones. You're allowed to initialize the parameters of the SVM model however you want, as long as your implemented functions return the right values.\n","- You'll need to play with the values of step size and number of iterations to\n","converge to a good value.\n","- To debug your optimization, print the objective over iterations. Remember that the theory says as long as the learning rate is small enough, for strongly convex problems, we are guaranteed to converge at a certain rate. What does this imply about your solution if it is not converging?\n","- As a sanity check, you can get around 97.5% prediction accuracy and converge to an objective below 0.16.  "],"metadata":{"id":"f7ETKg-L98OG"}},{"cell_type":"code","source":["from scipy.optimize._lsq.dogbox import LinearOperator\n","class SoftSVM():\n","    def __init__(self, ndims):\n","        # Here, we initialize the parameters of your soft-SVM model for binary\n","        # classification. Don't change the weight and bias variables as the\n","        # autograder will assume that these exist.\n","        # ndims := integer -- number of dimensions\n","        # no return type\n","\n","        self.weight = torch.zeros(ndims)\n","        self.bias = torch.zeros(1)\n","        self.weight.requires_grad = True\n","        self.bias.requires_grad = True\n","\n","    def objective(self, X, y, l2_reg):\n","        # Calculate the objective of your soft-SVM model\n","        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n","        # y := Tensor of size (m) -- the labels for each example in X\n","        # l2_reg := float -- L2 regularization penalty\n","        # Returns a scalar tensor (zero dimensional tensor) -- the loss for the model\n","        # Fill in the rest\n","        hinge = torch.mean(torch.max(1 - y * (X @ self.weight + self.bias), torch.zeros(1)))\n","        reg = l2_reg * torch.norm(self.weight, p=2)**2\n","        obj = hinge + reg\n","\n","        return obj\n","\n","\n","    def gradient(self, X, y, l2_reg):\n","        # Calculate the gradient of your soft-SVM model\n","        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n","        # y := Tensor of size (m) -- the labels for each example in X\n","        # l2_reg := float -- L2 regularization penalty\n","        # Return Tuple (Tensor, Tensor) -- the tensors corresponds to the weight\n","        # and bias parameters respectively\n","        # Fill in the rest\n","        hinge = torch.max(1 - y * (X @ self.weight + self.bias), torch.zeros(1))\n","        hinge_grad = -y * (hinge > 0).float()\n","        reg_grad = 2 * l2_reg * self.weight\n","        weight_grad = (X.T @ hinge_grad) / X.shape[0] + reg_grad\n","        bias_grad = torch.sum(hinge_grad) / X.shape[0]\n","\n","        return weight_grad, bias_grad\n","\n","    def optimize(self, X, y, l2_reg):\n","        # Calculate the gradient of your soft-SVM model\n","        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n","        # y := Tensor of size (m) -- the labels for each example in X\n","        # l2_reg := float -- L2 regularization penalty\n","\n","        # no return type\n","\n","        # Fill in the rest\n","        for _ in range(10000):\n","            obj = self.objective(X, y, l2_reg)\n","            weight_grad, bias_grad = self.gradient(X, y, l2_reg)\n","            self.weight.data -= 0.001 * weight_grad\n","            self.bias.data -= 0.001 * bias_grad\n","\n","\n","    def predict(self, X):\n","        # Given an X, make a prediction with the SVM\n","        # X := Tensor of size (m,d) -- features of m examples with d dimensions\n","        # Return a tensor of size (m) -- the prediction labels on the dataset X\n","\n","        # Fill in the rest\n","        pred= torch.sign(X @ self.weight + self.bias)\n","        return pred"],"metadata":{"id":"d8oyIkeG-LzH","executionInfo":{"status":"ok","timestamp":1697611229914,"user_tz":240,"elapsed":111,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"execution_count":221,"outputs":[]},{"cell_type":"code","source":["from sklearn import datasets\n","\n","#Load dataset\n","cancer = datasets.load_breast_cancer()\n","X,y = torch.from_numpy(cancer['data']), torch.from_numpy(cancer['target'])\n","mu,sigma = X.mean(0,keepdim=True), X.std(0,keepdim=True)\n","X,y = ((X-mu)/sigma).float(),(y - 0.5).sign() # prepare data\n","l2_reg = 0.1\n","print(X.size(), y.size())\n","\n","# Optimize the soft-SVM with gradient descent\n","clf = SoftSVM(X.size(1))\n","clf.optimize(X,y,l2_reg)\n","print(\"\\nSoft SVM objective: \")\n","print(clf.objective(X,y,l2_reg).item())\n","print(\"\\nSoft SVM accuracy: \")\n","(clf.predict(X) == y).float().mean().item()"],"metadata":{"id":"FntYD7Jy-P9U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697611233955,"user_tz":240,"elapsed":2943,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"95645ff7-6527-4f89-a492-e35de4625b02"},"execution_count":222,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([569, 30]) torch.Size([569])\n","\n","Soft SVM objective: \n","0.15918760001659393\n","\n","Soft SVM accuracy: \n"]},{"output_type":"execute_result","data":{"text/plain":["0.9753954410552979"]},"metadata":{},"execution_count":222}]},{"cell_type":"markdown","source":["### Autograder\n","Be sure you can pass the following four test cases!"],"metadata":{"id":"4TfVUYlF-iKC"}},{"cell_type":"code","source":["grader.grade(test_case_id = 'SVM_objective', answer = SoftSVM)\n","grader.grade(test_case_id = 'SVM_gradient', answer = SoftSVM)\n","grader.grade(test_case_id = 'SVM_optimize', answer = SoftSVM)\n","grader.grade(test_case_id = 'SVM_predict', answer = SoftSVM)"],"metadata":{"id":"fqpFzwc7-hwf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697611248206,"user_tz":240,"elapsed":12593,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"fdd66971-9b1e-4e27-c597-8d50d6d35b1f"},"execution_count":223,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct! You earned 2/2 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 2/2 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 4/4 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 2/2 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n"]}]},{"cell_type":"markdown","source":["# Submitting to Gradescope\n","Before submitting to Gradescope, make sure that selecting \"Runtime\" -> \"Restart and run all\" completes all cells without errors.\n","\n","1. Go to the File menu and choose \"Download .ipynb\" and also \"Download .py\". Make sure these files are named homework3.ipynb and homework3.py, respectively\n","2. Go to GradeScope through the canvas page and ensure your class is \"BAN_CIS-5200-001 202330\"\n","3. Select Homework 2\n","4. Upload both files (the .ipynb and the .py)\n","5. PLEASE CHECK THE AUTOGRADER OUTPUT TO ENSURE YOUR SUBMISSION IS PROCESSED CORRECTLY! If this is the case, you should be all set with the programming component of this homework!"],"metadata":{"id":"9H3yJsDc-8Z0"}}]}