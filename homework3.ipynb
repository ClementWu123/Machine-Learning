{"cells":[{"cell_type":"markdown","metadata":{"id":"m1dkS0VksGp7"},"source":["# CIS 5200: Machine Learning\n","## Homework 3"]},{"cell_type":"code","execution_count":118,"metadata":{"id":"QXZxyZDbr-Yx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699071904580,"user_tz":240,"elapsed":133,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"d1933731-cf71-4d4b-8189-cc30d0a75c55"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO, OK] Google Colab.\n"]}],"source":["import os\n","import sys\n","\n","# For autograder only, do not modify this cell.\n","# True for Google Colab, False for autograder\n","NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n","if NOTEBOOK:\n","    print(\"[INFO, OK] Google Colab.\")\n","else:\n","    print(\"[INFO, OK] Autograder.\")\n","    sys.exit()"]},{"cell_type":"markdown","metadata":{"id":"swECpqQGvLu9"},"source":["### Penngrader setup"]},{"cell_type":"code","execution_count":119,"metadata":{"id":"-peqcQNCvFSS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699071909458,"user_tz":240,"elapsed":4738,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"16e7e8ee-c9d7-40e3-b9d2-7bd852c619e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: penngrader-client in /usr/local/lib/python3.10/dist-packages (0.5.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from penngrader-client) (0.3.7)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from penngrader-client) (6.0.1)\n"]}],"source":["# %%capture\n","!pip install penngrader-client"]},{"cell_type":"code","execution_count":120,"metadata":{"id":"9VOzgVapPgrZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699071909459,"user_tz":240,"elapsed":4,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"4c0e404a-42e5-4df9-c264-42045de49cb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting config.yaml\n"]}],"source":["%%writefile config.yaml\n","grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n","grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"]},{"cell_type":"code","execution_count":121,"metadata":{"id":"SRSiFAHsu0UQ","executionInfo":{"status":"ok","timestamp":1699071909459,"user_tz":240,"elapsed":3,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"outputs":[],"source":["# packages for homework\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"yK9J863mtwKz"},"source":["### 1. Decision Trees and Bagging\n","\n","In this problem, we'll implement a simplified version of random forests. We'll be using the `iris` dataset, which has 4 features that are discretized to $0.1$ steps between $0$ and $8$ (i.e. the set of all possible features is $\\{0.1, 0.2, \\dots, 7.8, 7.9\\}$. Thus, all thresholds that we'll need to consider are $\\{0.15, 0.25, \\dots, 7.75, 7.85\\}$.\n","\n","Your task in this first part is to finish the implementation of decision trees. We've provided a template for some of the decision tree code, following which you'll finish the bagging algorithm to get a random forest.\n","\n","1. Entropy (2pts): calculate the entropy of a given vector of labels in the `entropy` function. Note that the generalization of entropy to 3 classes is $H = -\\sum_{i=1}^3 p_i\\log(p_i)$ where $p_i$ is the proportion of examples with label $i$.\n","2. Find the best split (1pt): finish the `find_split` function by finding the feature index and value that results in the split minimizing entropy.\n","3. Build the tree (2pts): finish the `expand_node` function by completing the recursive call for building the decision tree.\n","4. Predict with tree (2pts): implement the `predict_one` function, which makes a prediction for a single example.\n","\n","Throughout these problems, the way we represent the decision tree is by using python dicts. In particular, a node is a `dict` that can have the following keys:\n","\n","1. `node['label']` Return the label that we should predict upon reaching this node. node should **only** have a `label` entry if it is a leaf node.\n","2. `node['left']` points to another node dict that represents this node's left child.\n","3. `node['right']` points to another node dict that represents this node's right child.\n","4. `node['split']` is a tuple containing the feature index and value that this node splits left versus right on.\n","\n","In our implementation, all comparisons will be **greater than** comparisons, and \"yes\" answers go **left**. In other words, if `node['split'] = (2, 1.25)`, then we expect to find all data remaining at this node with feature 2 value **greater** than 1.25 in `node['left']`, and feature value **less** than 1.25 in `node['right']`.\n","\n","Tips:\n","+ If you have NaNs, you may be dividing by zero.\n"]},{"cell_type":"code","execution_count":122,"metadata":{"id":"M0c0WAqatpwW","executionInfo":{"status":"ok","timestamp":1699071909459,"user_tz":240,"elapsed":3,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"outputs":[],"source":["def entropy(y):\n","    # Calculate the entropy of a given vector of labels in the `entropy` function.\n","    #\n","    # y := Tensor(int) of size (m) -- the given vector of labels\n","    # Return a float that is your calculated entropy\n","\n","    # Fill in the rest\n","    _, count = torch.unique(y, return_counts=True)\n","    prob = count.float() / len(y)\n","    entropy = -torch.sum(prob * torch.log2(prob))\n","    return entropy.item()\n","\n","def find_split(node, X, y, k=4):\n","    # Find the best split over all possible splits that minimizes entropy.\n","    #\n","    # node := Map(string: value) -- the tree represented as a Map, the key will take four\n","    #   different string: 'label', 'split','left','right' (See implementation below)\n","    #   'label': a label node with value as the mode of the labels\n","    #   'split': the best split node with value as a tuple of feature id and threshold\n","    #   'left','right': the left and right branch with value as the label node\n","    # X := Tensor of size (m,d) -- Batch of m examples of demension d\n","    # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n","    # k := int -- the number of classes, with default value as 4\n","    # Return := tuple of (int, float) -- the feature id and threshold of the best split\n","\n","    m = y.size(0)\n","    best_H, best_split = 999, None\n","    features = torch.randint(0, 4,(k,))\n","\n","    for feature_idx in features:\n","        for threshold in torch.arange(0.15,7.9,0.1):\n","            idx = X[:,feature_idx] > threshold\n","\n","            if idx.sum() == 0 or idx.sum() == idx.size(0):\n","                continue\n","\n","            m_left = idx.sum()\n","            m_right = (~idx).sum()\n","\n","            H_left = entropy(y[idx])\n","            H_right = entropy(y[~idx])\n","\n","            ## ANSWER\n","            split_H = (m_left/m)*H_left + (m_right/m)*H_right\n","            ## END ANSWER\n","\n","            if split_H < best_H or best_split == None:\n","                best_H, best_split = split_H, (feature_idx, threshold)\n","    return best_split\n","\n","def expand_node(node, X, y, max_depth=0, k=4):\n","    # Completing the recursive call for building the decision tree\n","    # node := Map(string: value) -- the tree represented as a Map, the key will take four\n","    #   different string: 'label', 'split','left','right' (See implementation below)\n","    #   'label': a label node with value as the mode of the labels\n","    #   'split': the best split node with value as a tuple of feature id and threshold\n","    #   'left','right': the left and right branch with value as the label node\n","    # X := Tensor of size (m,d) -- Batch of m examples of demension d\n","    # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n","    # max_depth := int == the deepest level of the the decision tree\n","    # k := int -- the number of classes, with default value as 4\n","    # Return := tuple of (int, float) -- the feature id and threshold of the best split\n","    #\n","\n","    H = entropy(y)\n","    if H == 0 or max_depth == 0:\n","        return\n","\n","    best_split = find_split(node, X, y, k=k)\n","\n","    ####################################\n","    # THIS LINE BELOW WILL REMOVE UNKNOWN OPCODE\n","    ####################################\n","    if best_split == None:\n","        return\n","\n","    idx = X[:,best_split[0]] > best_split[1]\n","    X_left, y_left = X[idx], y[idx]\n","    X_right, y_right = X[~idx], y[~idx]\n","\n","    del node['label']\n","    node['split'] = best_split\n","    node['left'] = { 'label': y_left.mode().values }\n","    node['right'] = { 'label': y_right.mode().values }\n","\n","    ## ANSWER\n","    expand_node(node['left'], X_left, y_left, max_depth - 1, k)\n","    expand_node(node['right'], X_right, y_right, max_depth - 1, k)\n","    ## END ANSWER\n","    return\n","\n","def fit_decision_tree(X,y, k=4):\n","    # The function will fit data with decision tree with the expand_node method implemented above\n","\n","    root = { 'label': y.mode().values }\n","    expand_node(root, X, y, max_depth=10, k=k)\n","    return root\n","\n","def predict_one(node, x):\n","    # Makes a prediction for a single example.\n","    # node := Map(string: value) -- the tree represented as a Map, the key will take four\n","    #   different string: 'label', 'split','left','right' (See implementation below)\n","    #   'label': a label node with value as the mode of the labels\n","    #   'split': the best split node with value as a tuple of feature id and threshold\n","    #   'left','right': the left and right branch with value as the label node\n","    # x := Tensor(float) of size(d,) -- the single example in a batch\n","    # Fill in the rest\n","    if 'label' in node:\n","        return node['label']\n","    feature_id, threshold = node['split']\n","    if x[feature_id] > threshold:\n","        return predict_one(node['left'], x)\n","    else:\n","        return predict_one(node['right'], x)\n","\n","def predict(node, X, predict_one=predict_one):\n","    # return the predict result of the entire batch of examples using the predict_one function above.\n","    return torch.stack([predict_one(node, x) for x in X])"]},{"cell_type":"markdown","metadata":{"id":"G-Lyl-r5CO2k"},"source":["Test your code on the `iris` dataset. Your decision tree should fit to 100\\% training accuracy and generalize to 88\\% test accuracy."]},{"cell_type":"code","execution_count":123,"metadata":{"id":"k1CZzJ_GLbUE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699071909631,"user_tz":240,"elapsed":175,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"7067923e-6f77-41e1-967d-2bd38b132169"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy:  1.0\n","Test accuracy:  0.9200000166893005\n"]}],"source":["iris = datasets.load_iris()\n","data=train_test_split(iris.data,iris.target,test_size=0.5,random_state=123)\n","\n","X,X_te,y,y_te = [torch.from_numpy(A) for A in data]\n","X,X_te,y,y_te = X.float(), X_te.float(), y.long(), y_te.float()\n","\n","DT = fit_decision_tree(X,y,k=4)\n","\n","print('Train accuracy: ', (predict(DT, X) == y).float().mean().item())\n","print('Test accuracy: ', (predict(DT, X_te) == y_te).float().mean().item())"]},{"cell_type":"markdown","metadata":{"id":"slw1r1EInAl6"},"source":["### Bagging Decision Trees for Random forests\n","\n","Note that our `find_split` implementation can use a random subset of the features when searching for the right split via the argument $k$. For the vanilla decision tree, we defaulted to $k=4$. Since there were 4 features, this meant that the decision tree could always considered all 4 features. By reducing the value of $k$ to $\\sqrt(k)=2$, we can introduce variance into the decision trees for the bagging algorithm.\n","\n","You'll now implement the bagging algorithm. Note that if you use the `clf` and `predict` functions given as keyword arguments, you can pass this section in the autograder without needing a correct implementation for decision trees from the previous section.\n","\n","1. Bootstrap (1pt): Implement `bootstrap` to draw a random bootstrap dataset from the given dataset.\n","2. Fitting a random forest (1pt): Implement `random_forest_fit` to train a random forest that fits the data.\n","3. Predicting with a random forest (1pt): Implement `predict_forest_fit` to make predictions given a random forest.\n","\n","Tip:\n","+ If you're not sure whether your bootstrap is working or not, remember that on average, there will be $1-1/e\\approx 0.632$ unique samples in a bootstrapped dataset."]},{"cell_type":"code","execution_count":124,"metadata":{"id":"2tjqsx8cc7A3","executionInfo":{"status":"ok","timestamp":1699071909631,"user_tz":240,"elapsed":1,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"outputs":[],"source":["def bootstrap(X,y):\n","    # Draw a random bootstrap dataset from the given dataset.\n","    #\n","    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n","    # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n","    #\n","    # Return := Tuple of (Tensor(float) of size (m,d),Tensor(int) of size(m,)) -- the random bootstrap\n","    #       dataset of X and its correcting lable Y\n","    # Fill in the rest\n","    m, d = X.size()\n","    index = torch.randint(0, m, (m,))  # Generate random indices with replacement\n","    X_bs = X[index]\n","    y_bs = y[index]\n","\n","    return X_bs, y_bs\n","\n","def random_forest_fit(X, y, m, k, clf=fit_decision_tree):\n","    # Train a random forest that fits the data.\n","    # X := Tensor(float) of size (n,d) -- Batch of n examples of demension d\n","    # y := Tensor(int) of size (n) -- the given vectors of labels of the examples\n","    # m := int -- number of trees in the random forest\n","    # k := int -- number of classes of the features\n","    # clf := function -- the decision tree model that the data will be trained on\n","    #\n","    # Return := the random forest generated from the training datasets\n","    # Fill in the rest\n","    forest = []\n","    n, d = X.size()\n","    for i in range(m):\n","        X_bs, y_bs = bootstrap(X,y)\n","        tree = clf(X_bs, y_bs, k)\n","        forest.append(tree)\n","\n","    return forest\n","\n","def random_forest_predict(X, clfs, predict=predict):\n","    # Implement `predict_forest_fit` to make predictions given a random forest.\n","    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n","    # clfs := list of functions -- the random forest\n","    # Return := Tensor(int) of size (m,) -- the predicted label from the random forest\n","    # Fill in the rest\n","    result = []\n","    for clf in clfs:\n","        pred = predict(clf, X)\n","        result.append(pred)\n","\n","    result = torch.stack(result)\n","    predicted, _ = torch.mode(result, dim=0)\n","    return predicted"]},{"cell_type":"markdown","metadata":{"id":"W_MAYCuQEKej"},"source":["Test your code again on the `iris` dataset. Our random forest was able to improve the accuracy of the decision tree by about 10\\%!"]},{"cell_type":"code","execution_count":125,"metadata":{"id":"tpFa5b_abG4S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699071918890,"user_tz":240,"elapsed":9260,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"8f564a27-0cf0-4c1e-dc74-d480272fb594"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy:  1.0\n","Test accuracy:  0.9733333587646484\n"]}],"source":["torch.manual_seed(42)\n","RF = random_forest_fit(X,y,50,2)\n","\n","print('Train accuracy: ', (random_forest_predict(X, RF) == y).float().mean().item())\n","print('Test accuracy: ', (random_forest_predict(X_te, RF) == y_te).float().mean().item())"]},{"cell_type":"markdown","metadata":{"id":"eqz8OEmbqacx"},"source":["As a sanity check, the random forest can get around 95-97% test accuracy."]},{"cell_type":"markdown","metadata":{"id":"W7LWmjB_64uO"},"source":["# 1. Boosting\n","\n","In this problem, you'll implement a basic boosting algorithm on the binary classification breast cancer dataset. Here, we've provided the following weak learner for you: an $\\ell_2$ regularized logistic classifier trained with gradient descent."]},{"cell_type":"code","execution_count":126,"metadata":{"id":"_pJ8cun0tHAR","executionInfo":{"status":"ok","timestamp":1699071918890,"user_tz":240,"elapsed":3,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"outputs":[],"source":["class Logistic(nn.Module):\n","    def __init__(self):\n","        super(Logistic, self).__init__()\n","        self.linear = nn.Linear(30,1)\n","    def forward(self, X):\n","        out = self.linear(X)\n","        return out.squeeze()\n","\n","def fit_logistic_clf(X,y):\n","    clf = Logistic()\n","    opt = torch.optim.Adam(clf.parameters(), lr=0.1, weight_decay=1e2)\n","    loss = torch.nn.BCEWithLogitsLoss()\n","    for t in range(200):\n","        out = clf(X)\n","        opt.zero_grad()\n","        loss(out,(y>0).float()).backward()\n","        # if t % 50 == 0:\n","        #     print(loss(out,y.float()).item())\n","        opt.step()\n","    return clf\n","\n","def predict_logistic_clf(X, clf):\n","    return torch.sign(clf(X)).squeeze()"]},{"cell_type":"markdown","metadata":{"id":"R-rk0eIKEmwM"},"source":["Your task is to boost this logistic classifier to reduce its bias. Implement the following two functions:\n","\n","+ Finish the boosting algorithm: we've provided a template for the boosting algorithm in `boosting_fit`, however it is missing several components. Fill in the missing snippets of code.\n","+ Prediction after boosting (2pts): implement `boosting_predict` to make predictions with a given boosted model."]},{"cell_type":"code","execution_count":137,"metadata":{"id":"zDswQCyYwp4E","executionInfo":{"status":"ok","timestamp":1699072341799,"user_tz":240,"elapsed":151,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"outputs":[],"source":["def boosting_fit(X,y, T=10):\n","    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n","    # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n","    # T := Maximum number of models to be implemented\n","\n","    m = X.size(0)\n","    clfs = []\n","\n","    while len(clfs) < T:\n","        # Calculate the weights for each sample mu. You may need to\n","        # divide this into the base case and the inductive case.\n","        # mu = ...\n","\n","        ## ANSWER\n","        if clfs == []:\n","          mu = torch.ones(m) / m\n","        else:\n","          mu = mu * torch.exp(alpha * misclassified)\n","          mu = mu / torch.sum(mu)\n","        ## END ANSWER\n","\n","        # Here, we draw samples according to mu and fit a weak classifier\n","        idx = torch.multinomial(mu, m, replacement=True)\n","        X0, y0 = X[idx], y[idx]\n","\n","        clf = fit_logistic_clf(X0, y0)\n","\n","        # Calculate the epsilon error term\n","        # eps = ...\n","\n","        ## ANSWER\n","        pred = predict_logistic_clf(X, clf)\n","        misclassified = (pred != y).float()\n","        eps = torch.sum(mu * misclassified)\n","        ## END ANSWER\n","\n","        if eps > 0.5:\n","            # In the unlikely even that gradient descent fails to\n","            # find a good classifier, we'll skip this one and try again\n","            continue\n","\n","        # Calculate the alpha term here\n","        # alpha = ...\n","\n","        ## ANSWER\n","        alpha = 0.5 * torch.log2((1 - eps) / eps)\n","        ## END ANSWER\n","\n","\n","        clfs.append((alpha,clf))\n","    return clfs\n","\n","def boosting_predict(X, clfs):\n","    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n","    # clfs := list of tuples of (float, logistic classifier) -- the list of boosted classifiers\n","    # Return := Tnesor(int) of size (m) -- the predicted labels of the dataset\n","    return torch.sign(sum(alpha*predict_logistic_clf(X, clf) for alpha,clf in clfs))\n"]},{"cell_type":"markdown","metadata":{"id":"JmbKld8TGk8y"},"source":["Test out your code on the breast cancer dataset. As a sanity check, your statndard logistic classifier will get a train/test accuracy of around 84%-88% while the boosted logistic classifier will get a train/test accuracy of around 90%."]},{"cell_type":"code","execution_count":138,"metadata":{"id":"ppRNBbiY84N5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699072345753,"user_tz":240,"elapsed":2638,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"389493b5-110f-4369-b05e-7a3794fd8025"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic classifier accuracy:\n","Train accuracy:  0.8351648449897766\n","Test accuracy:  0.7894737124443054\n","Boosted logistic classifier accuracy:\n","Train accuracy:  0.8703296780586243\n","Test accuracy:  0.9035087823867798\n"]}],"source":["from sklearn.datasets import load_breast_cancer\n","cancer = datasets.load_breast_cancer()\n","data=train_test_split(cancer.data,cancer.target,test_size=0.2,random_state=123)\n","\n","torch.manual_seed(123)\n","\n","X,X_te,y,y_te = [torch.from_numpy(A) for A in data]\n","X,X_te,y,y_te = X.float(), X_te.float(), torch.sign(y.long()-0.5), torch.sign(y_te.long()-0.5)\n","\n","\n","logistic_clf = fit_logistic_clf(X,y)\n","print(\"Logistic classifier accuracy:\")\n","print('Train accuracy: ', (predict_logistic_clf(X, logistic_clf) == y).float().mean().item())\n","print('Test accuracy: ', (predict_logistic_clf(X_te, logistic_clf) == y_te).float().mean().item())\n","\n","boosting_clfs = boosting_fit(X,y)\n","print(\"Boosted logistic classifier accuracy:\")\n","print('Train accuracy: ', (boosting_predict(X, boosting_clfs) == y).float().mean().item())\n","print('Test accuracy: ', (boosting_predict(X_te, boosting_clfs) == y_te).float().mean().item())"]},{"cell_type":"markdown","metadata":{"id":"Buoxh78o5j0p"},"source":["## Autograder"]},{"cell_type":"code","execution_count":139,"metadata":{"id":"iZF-CM9XBHFf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699072358455,"user_tz":240,"elapsed":10636,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}},"outputId":"155d5c30-c342-4b08-92b7-a50cd3e956b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["PennGrader initialized with Student ID: 72249835\n","\n","Make sure this correct or we will not be able to store your grade\n","Correct! You earned 2/2 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 1/1 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 2/2 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 2/2 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 1/1 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 1/1 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 1/1 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 3/3 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n","Correct! You earned 2/2 points. You are a star!\n","\n","Your submission has been successfully recorded in the gradebook.\n"]}],"source":["# Autograder will be announced on Ed Discussion approximately a week after initial release\n","from penngrader.grader import PennGrader\n","\n","# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n","# TO ASSIGN POINTS TO YOU IN OUR BACKEND\n","STUDENT_ID = 72249835 # YOUR PENN-ID GOES HERE AS AN INTEGER #\n","SECRET = STUDENT_ID\n","\n","grader = PennGrader('config.yaml', 'CIS5200_FALL_2023_HW3', STUDENT_ID, SECRET)\n","\n","\n","grader.grade(test_case_id = 'entropy', answer = entropy)\n","grader.grade(test_case_id = 'find_split', answer = find_split)\n","grader.grade(test_case_id = 'expand_node', answer = expand_node)\n","grader.grade(test_case_id = 'predict_one', answer = predict_one)\n","\n","grader.grade(test_case_id = 'bootstrap', answer = bootstrap)\n","grader.grade(test_case_id = 'random_forest_fit', answer = random_forest_fit)\n","grader.grade(test_case_id = 'random_forest_predict', answer = random_forest_predict)\n","\n","grader.grade(test_case_id = 'boosting_fit', answer = boosting_fit)\n","grader.grade(test_case_id = 'boosting_predict', answer = boosting_predict)"]},{"cell_type":"code","source":[],"metadata":{"id":"-snUdve2AMIc","executionInfo":{"status":"ok","timestamp":1699071934604,"user_tz":240,"elapsed":3,"user":{"displayName":"Keqi Wu","userId":"01628003544885842630"}}},"execution_count":129,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1eJDm_GDZQDo7jv5HcDwMf9z3SeKaY6Al","timestamp":1699065017470}]},"kernelspec":{"display_name":"Python 3.10.1 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}